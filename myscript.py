import argparse
import collections
import json
import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path

import cv2
import numpy as np
import openslide
import pandas
from planar import BoundingBox, Vec2
from pymongo import MongoClient, errors
from shapely.geometry import Polygon, Point, MultiPoint
from skimage.color import separate_stains, hed_from_rgb


def assure_path_exists(path):
    """
    If path exists, great.
    If not, then create it.
    :param path:
    :return:
    """
    m_dir = os.path.dirname(path)
    if not os.path.exists(m_dir):
        os.makedirs(m_dir)


def mongodb_connect(client_uri):
    """
    Connection routine
    :param client_uri:
    :return:
    """
    try:
        return MongoClient(client_uri, serverSelectionTimeoutMS=1)
    except errors.ConnectionFailure:
        print("Failed to connect to server {}".format(client_uri))
        exit(1)


def get_file_list(substr, filepath):
    """
    Find lines in data file containing (case_id) substring.
    Return list.
    :param substr:
    :param filepath:
    :return:
    """
    lines = []
    with open(filepath) as f:
        for line in f:
            line = line.strip()
            if substr in line:
                lines.append(line)
    f.close()
    return lines


def copy_src_data(dest):
    """
    Copy data from nfs location to computation node.
    :param dest:
    :return:
    """
    # Get list of csv files containing features for this case_id
    for csv_dir1 in CSV_REL_PATHS:
        source_dir = os.path.join(CSV_FILE_PATH, csv_dir1)
        # copy all *.json and *features.csv files
        m_args = list(["rsync", "-ar", "--include", "*features.csv", "--include", "*.json"])
        # m_args = list(["rsync", "-avz", "--include", "*features.csv", "--include", "*.json"])
        m_args.append(source_dir)
        m_args.append(dest)
        print("executing " + ' '.join(m_args))
        subprocess.call(m_args)

    # Get slide
    my_file = Path(os.path.join(dest, (CASE_ID + '.svs')))
    if not my_file.is_file():
        svs_list = get_file_list(CASE_ID, 'config/image_path.list')
        svs_path = os.path.join(SVS_IMAGE_PATH, svs_list[0])
        print("executing scp", svs_path, dest)
        subprocess.check_call(['scp', svs_path, dest])


def get_tumor_markup(user_name):
    """
    Find what the pathologist circled as tumor.
    :param user_name:
    :return:
    """
    tumor_markup_list = []
    execution_id = (user_name + "_Tumor_Region")
    try:
        client = mongodb_connect('mongodb://' + args["db_host"] + ':27017')
        client.server_info()  # force connection, trigger error to be caught
        db = client.quip
        coll = db.objects
        filter_q = {
            'provenance.image.case_id': CASE_ID,
            'provenance.analysis.execution_id': execution_id
        }
        projection_q = {
            'geometry.coordinates': 1,
            '_id': 0
        }
        print('quip.objects')
        print(filter_q, ',', projection_q)
        cursor = coll.find(filter_q, projection_q)
        for item in cursor:
            # geometry.coordinates happens to be a list with one thing in it: a list! (of point coordinates).
            temp = item['geometry']['coordinates']  # [ [ [ x, y ], ... ] ]
            points = temp[0]  # [ [x, y ], ... ]
            tumor_markup_list.append(points)
        client.close()
    except errors.ServerSelectionTimeoutError as err:
        print('Error in get_tumor_markup', err)
        exit(1)

    count = len(tumor_markup_list)
    if count == 0:
        print('No tumor markups were generated by ', user_name)
        exit(1)

    print('Tumor markup count: ', count)
    return tumor_markup_list


def markup_to_polygons(markup_list):
    """
    Clean up and convert to something we can use.
    :param markup_list:
    :return:
    """
    m_poly_list = []
    try:
        # roll through our list of lists
        for coordinates in markup_list:
            points_list = []
            # convert the point coordinates to Points
            for m_point in coordinates:
                m_point = Point(m_point[0], m_point[1])
                points_list.append(m_point)
            # create a Polygon
            m = MultiPoint(points_list)
            m_polygon = Polygon(m)
            # append to return-list
            m_poly_list.append(m_polygon)
    except Exception as ex:
        print('Error in convert_to_polygons', ex)
        exit(1)

    # Return list of polygons
    return m_poly_list


def coordinates_to_polygons(coordinates_list):
    """
    Clean up and convert to something we can use.
    :param coordinates_list:
    :return:
    """
    m_poly_list = []
    points_list = []
    try:
        # roll through our list of [x,y]
        for m_point in coordinates_list:
            # convert the point coordinates to Points
            m_point = Point(m_point[0], m_point[1])
            points_list.append(m_point)
        # create a Polygon
        m = MultiPoint(points_list)
        m_polygon = Polygon(m)
        # append to return-list
        m_poly_list.append(m_polygon)
    except Exception as ex:
        print('Error in convert_to_polygons', ex)
        exit(1)

    # Return list of polygons
    return m_poly_list


def string_to_polygon(poly_data, imw, imh):
    """
    Convert Polygon string to polygon
    :param poly_data:
    :return:
    """
    points_list = []

    tmp_str = str(poly_data)
    tmp_str = tmp_str.replace('[', '')
    tmp_str = tmp_str.replace(']', '')
    split_str = tmp_str.split(':')
    m_polygon = {}

    try:
        # Get list of points
        for i in range(0, len(split_str) - 1, 2):
            a = float(split_str[i])
            b = float(split_str[i + 1])
            # Normalize points
            point = [a / float(imw), b / float(imh)]
            m_point = Point(point)
            points_list.append(m_point)
        # Create a Polygon
        m = MultiPoint(points_list)
        m_polygon = Polygon(m)
    except Exception as ex:
        print('Error in string_to_polygon', ex)
        exit(1)

    return m_polygon


def get_data_files():
    """
    Return 2 lists containing full paths for CSVs and JSONs.
    :return:
    """
    filenames = os.listdir(SLIDE_DIR)  # get all files' and folders' names in directory

    folders = []
    for filename in filenames:  # loop through all the files and folders
        ppath = os.path.join(os.path.abspath(SLIDE_DIR), filename)
        if os.path.isdir(ppath):  # check whether the current object is a folder or not
            folders.append(ppath)

    folders.sort()
    # print('subfolders: ', len(folders))

    json_files = []
    csv_files = []
    for index, filename in enumerate(folders):
        # print(index, filename)
        files = os.listdir(filename)
        for name in files:
            ppath = os.path.join(os.path.abspath(filename), name)
            if name.endswith('json'):
                json_files.append(ppath)
            elif name.endswith('csv'):
                csv_files.append(ppath)

    # print('json_files: ', len(json_files))
    # print('csv_files: ', len(csv_files))

    json_files.sort()
    csv_files.sort()
    return json_files, csv_files


def get_poly_within(jfiles, tumor_list):
    """
    Identify only the files within the tumor regions
    :param jfiles:
    :param tumor_list:
    :return:
    """
    # print('files len: ', len(jfiles))
    # print('tumor_list len: ', len(tumor_list))
    temp = {}
    path_poly = {}
    rtn_jfiles = []
    # start_time = time.time()
    pos = len('-algmeta.json')

    # Collect data
    for jfile in jfiles:
        with open(jfile, 'r') as f:
            # Read JSON data into the json_dict variable
            json_dict = json.load(f)
            # str = json_dict['out_file_prefix']
            imw = json_dict['image_width']
            imh = json_dict['image_height']
            tile_height = json_dict['tile_height']
            tile_width = json_dict['tile_width']
            tile_minx = json_dict['tile_minx']
            tile_miny = json_dict['tile_miny']
            inc_x = tile_minx + tile_width
            inc_y = tile_miny + tile_height
            # Create polygon for comparison
            point1 = Point(float(tile_minx) / float(imw), float(tile_miny) / float(imh))
            point2 = Point(float(inc_x) / float(imw), float(tile_miny) / float(imh))
            point3 = Point(float(inc_x) / float(imw), float(inc_y) / float(imh))
            point4 = Point(float(tile_minx) / float(imw), float(inc_y) / float(imh))
            point5 = Point(float(tile_minx) / float(imw), float(tile_miny) / float(imh))
            m = MultiPoint([point1, point2, point3, point4, point5])
            polygon = Polygon(m)
            # Map data file location (prefix) to bbox polygon
            path_poly[f.name[:-pos]] = polygon
        f.close()
        temp.update(path_poly)

    count = 0
    for tumor_roi in tumor_list:
        for key, val in temp.items():
            gotone = False
            if val.within(tumor_roi):
                gotone = True
                count += 1
            elif val.intersects(tumor_roi):
                gotone = True
                count += 1
            elif tumor_roi.within(val):
                gotone = True
                count += 1
            elif tumor_roi.intersects(val):
                gotone = True
                count += 1
            if gotone:
                rtn_jfiles.append(key)
                # rtn_obj.update({key: val})

    # elapsed_time = time.time() - start_time
    # print('Runtime get_poly_within: ')
    # print(time.strftime("%H:%M:%S", time.gmtime(elapsed_time)))

    return rtn_jfiles


def get_csv_data(files):
    """
    Get data
    :param files:
    :return:
    """
    start_time = time.time()
    obj_map = {}
    rtn_dict = {}

    for ff in files:
        jname = ff + '-algmeta.json'  # '-features.csv'
        with open(jname, 'r') as f:
            # Read JSON data into the json_dict variable
            json_dict = json.load(f)
            str = json_dict['out_file_prefix']
            imw = json_dict['image_width']
            imh = json_dict['image_height']
            tile_height = json_dict['tile_height']
            tile_width = json_dict['tile_width']
            tile_minx = json_dict['tile_minx']
            tile_miny = json_dict['tile_miny']

            cname = ff + '-features.csv'

            df = pandas.read_csv(cname)
            # print('df.shape[0]: ', df.shape[0])
            if df.empty:
                continue
            else:
                # new = old[['A', 'C', 'D']].copy()
                newdf = df[
                    ['AreaInPixels', 'Perimeter', 'Flatness', 'Circularity', 'r_GradientMean', 'b_GradientMean',
                     'b_cytoIntensityMean', 'r_cytoIntensityMean', 'r_IntensityMean', 'r_cytoGradientMean',
                     'Polygon']].copy()
                data_obj = {"df": newdf, "image_width": imw, "image_height": imh,
                            "tile_height": tile_height,
                            "tile_width": tile_width, "tile_minx": tile_minx, "tile_miny": tile_miny}
                obj_map[ff] = data_obj
        # Add to return variable
        rtn_dict.update(obj_map)
        f.close()

    elapsed_time = time.time() - start_time
    print('Runtime get_csv_data: ')
    print(time.strftime("%H:%M:%S", time.gmtime(elapsed_time)))

    return rtn_dict


def update_db(slide, df, val, name):
    """

    :param slide:
    :param df:
    :param val:
    :param name:
    :return:
    """
    # Mean (the simple average of the numbers)
    m_Perimeter = df['Perimeter'].mean()
    m_Flatness = df['Flatness'].mean()
    m_Circularity = df['Circularity'].mean()
    m_r_GradientMean = df['r_GradientMean'].mean()
    m_b_GradientMean = df['b_GradientMean'].mean()
    m_b_cytoIntensityMean = df['b_cytoIntensityMean'].mean()
    m_r_cytoIntensityMean = df['r_cytoIntensityMean'].mean()

    # Standard Deviation
    std_Perimeter = df['Perimeter'].std()
    std_Flatness = df['Flatness'].std()
    std_Circularity = df['Circularity'].std()
    std_r_GradientMean = df['r_GradientMean'].std()
    std_b_GradientMean = df['b_GradientMean'].std()
    std_b_cytoIntensityMean = df['b_cytoIntensityMean'].std()
    std_r_cytoIntensityMean = df['r_cytoIntensityMean'].std()

    # Ratio of nuclear material
    nucleus_area = df['AreaInPixels'].sum()
    percent_nuclear_material = compute_rnm(val['tile_width'], val['tile_height'], nucleus_area)
    print("Ratio of nuclear material: ", percent_nuclear_material)

    # Histology
    histological_data = histology(slide, val['tile_minx'], val['tile_miny'], val['tile_width'], val['tile_height'])
    print('histological_data', json.dumps(histological_data, indent=4, sort_keys=True))

    try:
        # client = mongodb_connect('mongodb://' + args["db_host"] + ':27017')
        # client.server_info()  # force connection, trigger error to be caught
        # db = client.quip_comp
        # collection_saved = db[name + '_features_td']  # name
        patch_feature_data = {}  # TODO: Remove when enabling db write.
        # patch_feature_data = collection_saved.OrderedDict()
        patch_feature_data['case_id'] = CASE_ID
        patch_feature_data['image_width'] = val['image_width']
        patch_feature_data['image_height'] = val['image_height']
        patch_feature_data['user'] = USER_NAME
        patch_feature_data['tile_minx'] = val['tile_minx']
        patch_feature_data['tile_miny'] = val['tile_miny']
        patch_feature_data['tile_width'] = val['tile_width']
        patch_feature_data['tile_height'] = val['tile_height']
        patch_feature_data['Flatness_segment_mean'] = m_Flatness
        patch_feature_data['Flatness_segment_std'] = std_Flatness
        patch_feature_data['Perimeter_segment_mean'] = m_Perimeter
        patch_feature_data['Perimeter_segment_std'] = std_Perimeter
        patch_feature_data['Circularity_segment_mean'] = m_Circularity
        patch_feature_data['Circularity_segment_std'] = std_Circularity
        patch_feature_data['r_GradientMean_segment_mean'] = m_r_GradientMean
        patch_feature_data['r_GradientMean_segment_std'] = std_r_GradientMean
        patch_feature_data['b_GradientMean_segment_mean'] = m_b_GradientMean
        patch_feature_data['b_GradientMean_segment_std'] = std_b_GradientMean
        patch_feature_data['r_cytoIntensityMean_segment_mean'] = m_r_cytoIntensityMean
        patch_feature_data['r_cytoIntensityMean_segment_std'] = std_r_cytoIntensityMean
        patch_feature_data['b_cytoIntensityMean_segment_mean'] = m_b_cytoIntensityMean
        patch_feature_data['b_cytoIntensityMean_segment_std'] = std_b_cytoIntensityMean
        print('patch_feature_data', json.dumps(patch_feature_data, indent=4, sort_keys=True))
        # patch_feature_data['datetime'] = datetime.now()
        # collection_saved.insert_one(patch_feature_data)

    except Exception as e:
        print('update_db: ', e)
        exit(1)


def calculate(data, is_patch):
    """
    Mean and std of Perimeter, Flatness, Circularity,
    r_GradientMean, b_GradientMean, b_cytoIntensityMean, r_cytoIntensityMean.
    :param data:
    :param is_patch: T/F (T=patch, F=patient)
    :return:
    """
    p = Path(os.path.join(SLIDE_DIR, (CASE_ID + '.svs')))
    print('Reading slide...')
    start_time = time.time()
    slide = openslide.OpenSlide(str(p))
    elapsed_time = time.time() - start_time
    print('Time it takes to read slide: ', elapsed_time)
    start_time = time.time()  # reset

    if is_patch:
        print('Calculating patch-level features...')
        # count = 0
        for key, val in data.items():
            df = val['df']
            # print('df.shape', df.shape[0])

            update_db(slide, df, val, 'patch')
            exit(0)  # TODO: TEST ONE.

    if not is_patch:
        print('Calculating patient-level features...')
        frames = []
        for key, val in data.items():
            frames.append(val['df'])
        result = pandas.concat(frames)
        update_db(slide, result, val, 'patient')

    slide.close()

    elapsed_time = time.time() - start_time
    print('Runtime calculate: ')
    print(time.strftime("%H:%M:%S", time.gmtime(elapsed_time)))


def test_db():
    try:
        name = 'test'
        client = mongodb_connect('mongodb://' + args["db_host"] + ':27017')
        client.server_info()  # force connection, trigger error to be caught
        db = client.quip_comp
        collection_saved = db[name + '_features_td']  # name
        patch_feature_data = collections.OrderedDict()
        patch_feature_data['test'] = 'test'
        patch_feature_data['datetime'] = datetime.now()
        collection_saved.insert_one(patch_feature_data)
    except Exception as e:
        print('test_db: ', e)
        exit(1)


def rgb_to_stain(rgb_img_matrix, sizex, sizey):
    """
    RGB to stain color space conversion
    :param rgb_img_matrix:
    :param sizex:
    :param sizey:
    :return:
    """
    hed_title_img = separate_stains(rgb_img_matrix, hed_from_rgb)
    hematoxylin_img_array = [[0 for x in range(sizex)] for y in range(sizey)]
    for index1, row in enumerate(hed_title_img):
        for index2, pixel in enumerate(row):
            hematoxylin_img_array[index1][index2] = pixel[0]

    return hematoxylin_img_array


def tile_operations(tile, type, name_prefix, w, h):
    """

    :param tile:
    :param type:
    :param name_prefix:
    :param w:
    :param h:
    :return:
    """
    data = {}

    img = tile.convert(type)

    # img to array
    img_array = np.array(img)

    if name_prefix == 'hematoxylin':
        # Convert rgb to stain color space
        img_array = rgb_to_stain(img_array, w, h)

    # average of the array elements
    tile_mean = np.mean(img_array)
    data[name_prefix + '_patch_mean'] = tile_mean

    # standard deviation of the array elements
    tile_std = np.std(img_array)
    data[name_prefix + '_patch_std'] = tile_std

    percentiles = [10, 25, 50, 75, 90]
    for i in range(len(percentiles)):
        name = name_prefix + '_patch_percentile_' + str(percentiles[i])
        data[name] = np.percentile(img_array, percentiles[i])
        # print(name_prefix + " patch {} percentile: {}".format(percentiles[i],
        # np.percentile(img_array, percentiles[i])))

    return data


def histology(slide, min_x, min_y, w, h):
    """

    :param slide:
    :param min_x:
    :param min_y:
    :param w:
    :param h:
    :return:
    """
    rtn_obj = {}
    try:
        # read_region returns an RGBA Image (PIL)
        tile = slide.read_region((min_x, min_y), 0, (w, h))

        # convert image and perform calculations
        a = tile_operations(tile, 'L', 'grayscale', w, h)
        b = tile_operations(tile, 'RGB', 'hematoxylin', w, h)
        c = {}

        for (key, value) in a.items():
            c.update({key: value})

        for (key, value) in b.items():
            c.update({key: value})

        rtn_obj = c

    except Exception as e:
        print('Error reading region: ', min_x, min_y)
        print(e)
        exit(1)

    return rtn_obj


def detect_bright_spots(gray):
    """
    Detect bright spots (no staining) and ignore those areas in area computation
    :param gray:
    :return:
    """
    # load the image, convert it to grayscale, and blur it
    # image = cv2.imread('img/detect_bright_spots.png')
    # gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    blurred = cv2.GaussianBlur(gray, (11, 11), 0)
    # Pixel values p >= 200 are set to 255 (white)
    # Pixel values < 200 are set to 0 (black).
    thresh = cv2.threshold(blurred, 200, 255, cv2.THRESH_BINARY)[1]

    # Do something.


def compute_rnm(width, height, total_polygon_area):
    """
    ratio of nuclear material
    :param width:
    :param height:
    :param total_polygon_area:
    :return:
    """
    area = width * height  # in pixels
    rnm = float(total_polygon_area / area)
    # Bridge is calculating area of polygon (computer_polygon.area), but area is in spreadsheet
    # percent_nuclear_material = float((nucleus_area / patch_polygon_area) * 100)
    return rnm


def doTiles(data):
    for key, val in data.items():
        df = val['df']
        width = val['patch_width']
        height = val['patch_height']
        minx = val['patch_minx']
        miny = val['patch_miny']

        cols = width / TILE_SIZE
        rows = height / TILE_SIZE

        data_complete = {}

        count = 0
        for x in range(1, (int(cols) + 1)):
            for y in range(1, (int(rows) + 1)):
                data = {}
                count += 1
                # minx = minx + (x * tile_size)
                # miny = miny + (y * tile_size)
                minx = x * TILE_SIZE
                miny = y * TILE_SIZE
                minx = minx + val['patch_minx']
                miny = miny + val['patch_miny']
                maxx = minx + TILE_SIZE
                maxy = miny + TILE_SIZE
                # print((minx, miny), (maxx, miny), (maxx, maxy), (minx, maxy))
                bbox = BoundingBox([(minx, miny), (maxx, miny), (maxx, maxy), (minx, maxy)])
                # print(bbox)
                row_list = []
                df2 = pandas.DataFrame()
                for index, row in df.iterrows():
                    poly_data = row['Polygon']
                    tmp_str = str(poly_data)
                    tmp_str = tmp_str.replace('[', '')
                    tmp_str = tmp_str.replace(']', '')
                    split_str = tmp_str.split(':')
                    # Get list of points
                    for i in range(0, len(split_str) - 1, 2):
                        a = float(split_str[i])
                        b = float(split_str[i + 1])
                        # point = [a, b]
                        point = Vec2(a, b)
                        if bbox.contains_point(point):
                            row_list.append(row)
                            df2.append(row)
                            break
                    # print('row_list', row_list)

                # data_complete.update({data[count]: {'bbox': bbox, 'row_list': row_list}})
                if row_list:
                    # data[count] = {'bbox': bbox, 'row_list': row_list}
                    data[count] = {'bbox': bbox, 'df2': df2}
                    data_complete.update(data)


# constant variables
WORK_DIR = "/data1/tdiprima/dataset"
CSV_FILE_PATH = "nfs004:/data/shared/bwang/composite_dataset"
SVS_IMAGE_PATH = "nfs001:/data/shared/tcga_analysis/seer_data/images"

# construct the argument parser and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-s", "--slide_name", help="svs image name")
ap.add_argument("-u", "--user_name", help="user who identified tumor regions")
ap.add_argument("-b", "--db_host", help="database host")
ap.add_argument("-t", "--tile_size", type=int, help="tile size")
args = vars(ap.parse_args())
print(args)

if not len(sys.argv) > 1:
    program_name = sys.argv[0]
    lst = ['python', program_name, '-h']
    subprocess.call(lst)  # Show help
    exit(1)

CASE_ID = args["slide_name"]
USER_NAME = args["user_name"]
TILE_SIZE = args["tile_size"]

SLIDE_DIR = os.path.join(WORK_DIR, CASE_ID) + os.sep
CSV_REL_PATHS = get_file_list(CASE_ID, 'config/csv_file_path.list')

# Fetch data.
assure_path_exists(SLIDE_DIR)
copy_src_data(SLIDE_DIR)

# Find what the pathologist circled as tumor.
tumor_mark_list = get_tumor_markup(USER_NAME)
# print('tumor_mark_list', len(tumor_mark_list))

# List of Tumor polygons
tumor_poly_list = markup_to_polygons(tumor_mark_list)
# print('tumor_poly_list', len(tumor_poly_list))

# Fetch list of data files
JSON_FILES, CSV_FILES = get_data_files()

# Identify only the files within the tumor regions
jfile_list = get_poly_within(JSON_FILES, tumor_poly_list)
# print('jfile_list len: ', len(jfile_list))

# Get data
csv_data = get_csv_data(jfile_list)
# print('csv_data len: ', len(csv_data))  # NOTE: s/b less b/c we ignore empty data files.

# Calculate
calculate(csv_data, True)
# calculate(csv_data, False)

exit(0)
