import argparse
import collections
import json
import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path

import cv2
import numpy as np
import openslide
import pandas
from planar import BoundingBox, Vec2
from pymongo import MongoClient, errors
from shapely.geometry import Polygon, Point, MultiPoint
from skimage.color import separate_stains, hed_from_rgb


def assure_path_exists(path):
    """
    If path exists, great.
    If not, then create it.
    :param path:
    :return:
    """
    m_dir = os.path.dirname(path)
    if not os.path.exists(m_dir):
        os.makedirs(m_dir)


def mongodb_connect(client_uri):
    """
    Connection routine
    :param client_uri:
    :return:
    """
    try:
        return MongoClient(client_uri, serverSelectionTimeoutMS=1)
    except errors.ConnectionFailure:
        print("Failed to connect to server {}".format(client_uri))
        exit(1)


def get_file_list(substr, filepath):
    """
    Find lines in data file containing (case_id) substring.
    Return list.
    :param substr:
    :param filepath:
    :return:
    """
    lines = []
    with open(filepath) as f:
        for line in f:
            line = line.strip()
            if substr in line:
                lines.append(line)
    f.close()
    return lines


def copy_src_data(dest):
    """
    Copy data from nfs location to computation node.
    :param dest:
    :return:
    """
    # Get list of csv files containing features for this case_id
    for csv_dir1 in DATA_FILE_SUBFOLDERS:
        source_dir = os.path.join(DATA_FILE_FOLDER, csv_dir1)
        # copy all *.json and *features.csv files
        m_args = list(["rsync", "-ar", "--include", "*features.csv", "--include", "*.json"])
        # m_args = list(["rsync", "-avz", "--include", "*features.csv", "--include", "*.json"])
        m_args.append(source_dir)
        m_args.append(dest)
        print("executing " + ' '.join(m_args))
        subprocess.call(m_args)

    # Get slide
    my_file = Path(os.path.join(dest, (CASE_ID + '.svs')))
    if not my_file.is_file():
        svs_list = get_file_list(CASE_ID, 'config/image_path.list')
        svs_path = os.path.join(SVS_IMAGE_FOLDER, svs_list[0])
        print("executing scp", svs_path, dest)
        subprocess.check_call(['scp', svs_path, dest])


def get_tumor_markup(user_name):
    """
    Find what the pathologist circled as tumor.
    :param user_name:
    :return:
    """
    tumor_markup_list = []
    execution_id = (user_name + "_Tumor_Region")
    try:
        client = mongodb_connect('mongodb://' + DB_HOST + ':27017')
        client.server_info()  # force connection, trigger error to be caught
        db = client.quip
        coll = db.objects
        filter_q = {
            'provenance.image.case_id': CASE_ID,
            'provenance.analysis.execution_id': execution_id
        }
        projection_q = {
            'geometry.coordinates': 1,
            '_id': 0
        }
        print('quip.objects')
        print(filter_q, ',', projection_q)
        cursor = coll.find(filter_q, projection_q)
        for item in cursor:
            # geometry.coordinates happens to be a list with one thing in it: a list! (of point coordinates).
            temp = item['geometry']['coordinates']  # [ [ [ x, y ], ... ] ]
            points = temp[0]  # [ [x, y ], ... ]
            tumor_markup_list.append(points)
        client.close()
    except errors.ServerSelectionTimeoutError as err:
        print('Error in get_tumor_markup', err)
        exit(1)

    count = len(tumor_markup_list)
    if count == 0:
        print('No tumor markups were generated by ', user_name)
        exit(1)

    print('Tumor markup count: ', count)
    return tumor_markup_list


def markup_to_polygons(markup_list):
    """
    Clean up and convert to something we can use.
    :param markup_list:
    :return:
    """
    m_poly_list = []
    try:
        # roll through our list of lists
        for coordinates in markup_list:
            points_list = []
            # convert the point coordinates to Points
            for m_point in coordinates:
                m_point = Point(m_point[0], m_point[1])
                points_list.append(m_point)
            # create a Polygon
            m = MultiPoint(points_list)
            m_polygon = Polygon(m)
            # append to return-list
            m_poly_list.append(m_polygon)
    except Exception as ex:
        print('Error in convert_to_polygons', ex)
        exit(1)

    # Return list of polygons
    return m_poly_list


def coordinates_to_polygons(coordinates_list):
    """
    Clean up and convert to something we can use.
    :param coordinates_list:
    :return:
    """
    m_poly_list = []
    points_list = []
    try:
        # roll through our list of [x,y]
        for m_point in coordinates_list:
            # convert the point coordinates to Points
            m_point = Point(m_point[0], m_point[1])
            points_list.append(m_point)
        # create a Polygon
        m = MultiPoint(points_list)
        m_polygon = Polygon(m)
        # append to return-list
        m_poly_list.append(m_polygon)
    except Exception as ex:
        print('Error in convert_to_polygons', ex)
        exit(1)

    # Return list of polygons
    return m_poly_list


def string_to_polygon(poly_data, imw, imh, normalize):
    """
    Convert Polygon string to polygon
    :param poly_data:
    :param imw:
    :param imh:
    :param normalize:
    :return:
    """
    points_list = []

    tmp_str = str(poly_data)
    tmp_str = tmp_str.replace('[', '')
    tmp_str = tmp_str.replace(']', '')
    split_str = tmp_str.split(':')
    m_polygon = {}

    try:
        # Get list of points
        for i in range(0, len(split_str) - 1, 2):
            a = float(split_str[i])
            b = float(split_str[i + 1])
            if normalize:
                # Normalize points
                point = [a / float(imw), b / float(imh)]
            else:
                point = [a, b]
            m_point = Point(point)
            points_list.append(m_point)
        # Create a Polygon
        m = MultiPoint(points_list)
        m_polygon = Polygon(m)
    except Exception as ex:
        print('Error in string_to_polygon', ex)
        exit(1)

    return m_polygon


def get_data_files():
    """
    Return 2 lists containing full paths for CSVs and JSONs.
    :return:
    """
    filenames = os.listdir(SLIDE_DIR)  # get all files' and folders' names in directory

    folders = []
    for filename in filenames:  # loop through all the files and folders
        ppath = os.path.join(os.path.abspath(SLIDE_DIR), filename)
        if os.path.isdir(ppath):  # check whether the current object is a folder or not
            folders.append(ppath)

    folders.sort()
    # print('subfolders: ', len(folders))

    json_files = []
    csv_files = []
    for index, filename in enumerate(folders):
        # print(index, filename)
        files = os.listdir(filename)
        for name in files:
            ppath = os.path.join(os.path.abspath(filename), name)
            if name.endswith('json'):
                json_files.append(ppath)
            elif name.endswith('csv'):
                csv_files.append(ppath)

    # print('json_files: ', len(json_files))
    # print('csv_files: ', len(csv_files))

    json_files.sort()
    csv_files.sort()
    return json_files, csv_files


def get_poly_within(jfiles, tumor_list):
    """
    Identify only the files within the tumor regions
    :param jfiles:
    :param tumor_list:
    :return:
    """
    # print('files len: ', len(jfiles))
    # print('tumor_list len: ', len(tumor_list))
    temp = {}
    path_poly = {}
    # rtn_jfiles = []
    rtn_obj = {}
    # start_time = time.time()

    # Collect data
    z = set()
    count = 0
    for jfile in jfiles:
        with open(jfile, 'r') as f:
            # Read JSON data into the json_dict variable
            json_dict = json.load(f)
            # str = json_dict['out_file_prefix']
            imw = json_dict['image_width']
            imh = json_dict['image_height']
            tile_height = json_dict['tile_height']
            tile_width = json_dict['tile_width']
            tile_minx = json_dict['tile_minx']
            tile_miny = json_dict['tile_miny']
            fp = json_dict['out_file_prefix']

            item = 'x' + str(tile_minx) + '_' + 'y' + str(tile_miny)
            if item not in z:  # If the object is not in the list yet...
                inc_x = tile_minx + tile_width
                inc_y = tile_miny + tile_height
                # Create polygon for comparison
                point1 = Point(float(tile_minx) / float(imw), float(tile_miny) / float(imh))
                point2 = Point(float(inc_x) / float(imw), float(tile_miny) / float(imh))
                point3 = Point(float(inc_x) / float(imw), float(inc_y) / float(imh))
                point4 = Point(float(tile_minx) / float(imw), float(inc_y) / float(imh))
                point5 = Point(float(tile_minx) / float(imw), float(tile_miny) / float(imh))
                m = MultiPoint([point1, point2, point3, point4, point5])
                polygon = Polygon(m)
                # Map data file location (prefix) to bbox polygon
                # path_poly[f.name[:-pos]] = polygon
                path_poly[item] = {'poly': polygon, 'image_width': imw, 'image_height': imh, 'tile_width': tile_width,
                                   'tile_height': tile_height, 'tile_minx': tile_minx, 'tile_miny': tile_miny,
                                   'out_file_prefix': fp}
            else:
                count += 1

            z.add(item)

        f.close()
        temp.update(path_poly)

    print('dupes', count)
    print('len', len(temp))

    for tumor_roi in tumor_list:
        for key, val in temp.items():
            gotone = False
            p = val['poly']
            if p.within(tumor_roi):
                gotone = True
            elif p.intersects(tumor_roi):
                gotone = True
            elif tumor_roi.within(p):
                gotone = True
            elif tumor_roi.intersects(p):
                gotone = True
            if gotone:
                # print('val', val)
                rtn_obj.update({key: val})

    # elapsed_time = time.time() - start_time
    # print('Runtime get_poly_within: ')
    # print(time.strftime("%H:%M:%S", time.gmtime(elapsed_time)))

    # return rtn_jfiles
    return rtn_obj


def aggregate_data(jfile_objs, CSV_FILES):
    """
    Get data
    :param jfile_objs:
    :param CSV_FILES
    :return:
    """
    start_time = time.time()
    obj_map = {}
    obj_map1 = {}
    rtn_dict = {}

    for k, v in jfile_objs.items():
        filelist = []
        for ff in CSV_FILES:
            if k in ff:
                filelist.append(ff)

        data_obj = {'filelist': filelist, "image_width": v['image_width'], "image_height": v['image_height'],
                    "tile_height": v['tile_height'], "tile_width": v['tile_width'], "tile_minx": v['tile_minx'],
                    "tile_miny": v['tile_miny']}
        obj_map.update({k: data_obj})

    print('obj_map', len(obj_map))
    print('Aggregating csv data...')

    for k, v in obj_map.items():
        frames = []
        for ff in v['filelist']:
            df = pandas.read_csv(ff)
            # print('df.shape[0]: ', df.shape[0])
            if df.empty:
                # print('empty!')
                # print(len(v['filelist']))
                # print(k)
                # print(ff)
                continue
            else:
                # new = old[['A', 'C', 'D']].copy()
                df1 = df[
                    ['AreaInPixels', 'Perimeter', 'Flatness', 'Circularity', 'r_GradientMean', 'b_GradientMean',
                     'b_cytoIntensityMean', 'r_cytoIntensityMean', 'r_IntensityMean', 'r_cytoGradientMean',
                     'Polygon']].copy()
                frames.append(df1)

        if frames:
            result = pandas.concat(frames)
            data_obj1 = {'df': result, "image_width": v['image_width'], "image_height": v['image_height'],
                         "tile_height": v['tile_height'], "tile_width": v['tile_width'], "tile_minx": v['tile_minx'],
                         "tile_miny": v['tile_miny']}

            obj_map1[ff] = data_obj1

        # Add to return variable
        rtn_dict.update(obj_map1)

    elapsed_time = time.time() - start_time
    print('Runtime aggregate_data: ')
    print(time.strftime("%H:%M:%S", time.gmtime(elapsed_time)))

    return rtn_dict


def update_db(slide, patch_data):
    """
    Write data.
    :param slide:
    :param patch_data:
    :return:
    """

    patch_index = patch_data['patch_num']
    df = patch_data['df']

    if df.empty:
        print('empty')
    else:
        # Ratio of nuclear material
        nucleus_area = df['AreaInPixels'].sum()
        # TODO: FIX
        percent_nuclear_material = compute_rnm(PATCH_SIZE, PATCH_SIZE, nucleus_area)
        print("Ratio of nuclear material: ", percent_nuclear_material)

        # Histology
        histological_data = histology(slide, patch_data['patch_minx'], patch_data['patch_miny'], PATCH_SIZE, PATCH_SIZE)
        print('histological_data', json.dumps(histological_data, indent=4, sort_keys=True))

        try:
            # client = mongodb_connect('mongodb://' + DB_HOST + ':27017')
            # client.server_info()  # force connection, trigger error to be caught
            # db = client.quip_comp
            # collection_saved = db[name + '_features_td']  # name
            patch_feature_data = {}  # TODO: Remove when enabling db write.
            # patch_feature_data = collection_saved.OrderedDict()
            patch_feature_data['case_id'] = CASE_ID
            patch_feature_data['image_width'] = patch_data['image_width']
            patch_feature_data['image_height'] = patch_data['image_height']
            patch_feature_data['user'] = USER_NAME
            patch_feature_data['tile_minx'] = patch_data['tile_minx']
            patch_feature_data['tile_miny'] = patch_data['tile_miny']
            patch_feature_data['patch_index'] = patch_index
            patch_feature_data['patch_minx'] = patch_data['patch_minx']
            patch_feature_data['patch_miny'] = patch_data['patch_miny']
            patch_feature_data['patch_width'] = PATCH_SIZE
            patch_feature_data['patch_height'] = PATCH_SIZE
            patch_feature_data['Flatness_segment_mean'] = df['Flatness'].mean()
            patch_feature_data['Flatness_segment_std'] = df['Flatness'].std()
            patch_feature_data['Perimeter_segment_mean'] = df['Perimeter'].mean()
            patch_feature_data['Perimeter_segment_std'] = df['Perimeter'].std()
            patch_feature_data['Circularity_segment_mean'] = df['Circularity'].mean()
            patch_feature_data['Circularity_segment_std'] = df['Circularity'].std()
            patch_feature_data['r_GradientMean_segment_mean'] = df['r_GradientMean'].mean()
            patch_feature_data['r_GradientMean_segment_std'] = df['r_GradientMean'].std()
            patch_feature_data['b_GradientMean_segment_mean'] = df['b_GradientMean'].mean()
            patch_feature_data['b_GradientMean_segment_std'] = df['b_GradientMean'].std()
            patch_feature_data['r_cytoIntensityMean_segment_mean'] = df['r_cytoIntensityMean'].mean()
            patch_feature_data['r_cytoIntensityMean_segment_std'] = df['r_cytoIntensityMean'].std()
            patch_feature_data['b_cytoIntensityMean_segment_mean'] = df['b_cytoIntensityMean'].mean()
            patch_feature_data['b_cytoIntensityMean_segment_std'] = df['b_cytoIntensityMean'].std()
            print('patch_feature_data', json.dumps(patch_feature_data, indent=4, sort_keys=True))
            # patch_feature_data['datetime'] = datetime.now()
            # collection_saved.insert_one(patch_feature_data)

        except Exception as e:
            print('Error in update_db: ', e)
            exit(1)


def calculate(tile_data):
    """
    Mean and std of Perimeter, Flatness, Circularity,
    r_GradientMean, b_GradientMean, b_cytoIntensityMean, r_cytoIntensityMean.
    :param tile_data:
    :return:
    """
    p = Path(os.path.join(SLIDE_DIR, (CASE_ID + '.svs')))
    print('Reading slide...')
    start_time = time.time()
    slide = openslide.OpenSlide(str(p))
    elapsed_time = time.time() - start_time
    print('Time it takes to read slide: ', elapsed_time)
    start_time = time.time()  # reset

    # Iterate through tile data
    for key, val in tile_data.items():
        # Create patches
        do_tiles(val, slide)
        exit(0)

    slide.close()

    elapsed_time = time.time() - start_time
    print('Runtime calculate: ')
    print(time.strftime("%H:%M:%S", time.gmtime(elapsed_time)))


def test_db():
    try:
        name = 'test'
        client = mongodb_connect('mongodb://' + DB_HOST + ':27017')
        client.server_info()  # force connection, trigger error to be caught
        db = client.quip_comp
        collection_saved = db[name + '_features_td']  # name
        patch_feature_data = collections.OrderedDict()
        patch_feature_data['test'] = 'test'
        patch_feature_data['datetime'] = datetime.now()
        collection_saved.insert_one(patch_feature_data)
    except Exception as e:
        print('test_db: ', e)
        exit(1)


def rgb_to_stain(rgb_img_matrix, sizex, sizey):
    """
    RGB to stain color space conversion
    :param rgb_img_matrix:
    :param sizex:
    :param sizey:
    :return:
    """
    hed_title_img = separate_stains(rgb_img_matrix, hed_from_rgb)
    hematoxylin_img_array = [[0 for x in range(sizex)] for y in range(sizey)]
    for index1, row in enumerate(hed_title_img):
        for index2, pixel in enumerate(row):
            hematoxylin_img_array[index1][index2] = pixel[0]

    return hematoxylin_img_array


def tile_operations(tile, type, name_prefix, w, h):
    """

    :param tile:
    :param type:
    :param name_prefix:
    :param w:
    :param h:
    :return:
    """
    data = {}

    img = tile.convert(type)

    # img to array
    img_array = np.array(img)

    if name_prefix == 'hematoxylin':
        # Convert rgb to stain color space
        img_array = rgb_to_stain(img_array, w, h)

    # average of the array elements
    tile_mean = np.mean(img_array)
    data[name_prefix + '_patch_mean'] = tile_mean

    # standard deviation of the array elements
    tile_std = np.std(img_array)
    data[name_prefix + '_patch_std'] = tile_std

    percentiles = [10, 25, 50, 75, 90]
    for i in range(len(percentiles)):
        name = name_prefix + '_patch_percentile_' + str(percentiles[i])
        data[name] = np.percentile(img_array, percentiles[i])
        # print(name_prefix + " patch {} percentile: {}".format(percentiles[i],
        # np.percentile(img_array, percentiles[i])))

    return data


def histology(slide, min_x, min_y, w, h):
    """

    :param slide:
    :param min_x:
    :param min_y:
    :param w:
    :param h:
    :return:
    """
    rtn_obj = {}
    try:
        # read_region returns an RGBA Image (PIL)
        tile = slide.read_region((min_x, min_y), 0, (w, h))

        # convert image and perform calculations
        a = tile_operations(tile, 'L', 'grayscale', w, h)
        b = tile_operations(tile, 'RGB', 'hematoxylin', w, h)
        c = {}

        for (key, value) in a.items():
            c.update({key: value})

        for (key, value) in b.items():
            c.update({key: value})

        rtn_obj = c

    except Exception as e:
        print('Error reading region: ', min_x, min_y)
        print(e)
        exit(1)

    return rtn_obj


def detect_bright_spots(gray):
    """
    Detect bright spots (no staining) and ignore those areas in area computation
    :param gray:
    :return:
    """
    # load the image, convert it to grayscale, and blur it
    # image = cv2.imread('img/detect_bright_spots.png')
    # gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    blurred = cv2.GaussianBlur(gray, (11, 11), 0)
    # Pixel values p >= 200 are set to 255 (white)
    # Pixel values < 200 are set to 0 (black).
    thresh = cv2.threshold(blurred, 200, 255, cv2.THRESH_BINARY)[1]

    # Do something.


def compute_rnm(width, height, total_polygon_area):
    """
    ratio of nuclear material
    :param width:
    :param height:
    :param total_polygon_area:
    :return:
    """
    area = width * height  # in pixels
    rnm = float(total_polygon_area / area)
    # percent_nuclear_material = float((nucleus_area / patch_polygon_area) * 100)
    return rnm


def do_tiles(data, slide):
    """
    Divide tile into patches
    :param data:
    :return:
    """
    print('Dividing patch into tiles...')
    start_time = time.time()

    patch_num = 0
    df = data['df']
    width = data['tile_width']
    height = data['tile_height']
    cols = width / PATCH_SIZE
    rows = height / PATCH_SIZE
    # data_complete = {}

    # Divide tile into patches
    for x in range(1, (int(cols) + 1)):
        for y in range(1, (int(rows) + 1)):
            patch_num += 1
            print('patch_num', patch_num)
            # minx = minx + (x * tile_size)
            # miny = miny + (y * tile_size)
            minx = x * PATCH_SIZE
            miny = y * PATCH_SIZE
            minx = minx + data['tile_minx']
            miny = miny + data['tile_miny']
            maxx = minx + PATCH_SIZE
            maxy = miny + PATCH_SIZE
            # Bounding box representing patch
            print((minx, miny), (maxx, miny), (maxx, maxy), (minx, maxy))
            # bbox = BoundingBox([(minx, miny), (maxx, miny), (maxx, maxy), (minx, maxy)])
            bbox = Polygon([(minx, miny), (maxx, miny), (maxx, maxy), (minx, maxy), (minx, miny)])
            row_list = []
            df2 = pandas.DataFrame()
            patch_area = 0.0
            # Figure out which polygons (data rows) belong to which patch
            for index, row in df.iterrows():
                xy = row['Polygon']
                polygon_shape = string_to_polygon(xy, data['image_width'], data['image_height'], False)
                # print('polygon_shape', polygon_shape)

                # Accumulate information
                if polygon_shape.within(bbox) or polygon_shape.intersects(bbox):
                    row_list.append(row)
                    df2 = df2.append(row)
                    if polygon_shape.intersects(bbox):
                        patch_area += polygon_shape.intersection(bbox).area
                    else:
                        patch_area += polygon_shape.area

            update_db(slide, {'df': df2, 'patch_area': patch_area, 'patch_num': patch_num,
                              'patch_minx': minx, 'patch_miny': miny, 'tile_minx': data['tile_minx'],
                              'tile_miny': data['tile_miny'], 'image_width': data['image_width'],
                              'image_height': data['image_height']})

    elapsed_time = time.time() - start_time
    print('Runtime do_tiles: ')
    print(time.strftime("%H:%M:%S", time.gmtime(elapsed_time)))


# constant variables
WORK_DIR = "/data1/tdiprima/dataset"
DATA_FILE_FOLDER = "nfs004:/data/shared/bwang/composite_dataset"
SVS_IMAGE_FOLDER = "nfs001:/data/shared/tcga_analysis/seer_data/images"

# construct the argument parser and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-s", "--slide_name", help="svs image name")
ap.add_argument("-u", "--user_name", help="user who identified tumor regions")
ap.add_argument("-b", "--db_host", help="database host")
ap.add_argument("-p", "--patch_size", type=int, help="patch size")
args = vars(ap.parse_args())
print(args)

if not len(sys.argv) > 1:
    program_name = sys.argv[0]
    lst = ['python', program_name, '-h']
    subprocess.call(lst)  # Show help
    exit(1)

CASE_ID = args["slide_name"]
USER_NAME = args["user_name"]
PATCH_SIZE = args["patch_size"]
DB_HOST = args["db_host"]

SLIDE_DIR = os.path.join(WORK_DIR, CASE_ID) + os.sep
DATA_FILE_SUBFOLDERS = get_file_list(CASE_ID, 'config/data_file_path.list')
# print('DATA_FILE_SUBFOLDERS', DATA_FILE_SUBFOLDERS)

# Fetch data.
assure_path_exists(SLIDE_DIR)
copy_src_data(SLIDE_DIR)

# Find what the pathologist circled as tumor.
tumor_mark_list = get_tumor_markup(USER_NAME)
# print('tumor_mark_list', len(tumor_mark_list))

# List of Tumor polygons
tumor_poly_list = markup_to_polygons(tumor_mark_list)
# print('tumor_poly_list', len(tumor_poly_list))

# Fetch list of data files
JSON_FILES, CSV_FILES = get_data_files()

# Identify only the files within the tumor regions
jfile_objs = get_poly_within(JSON_FILES, tumor_poly_list)
print('get_poly_within len: ', len(jfile_objs))

# Get data
csv_data = aggregate_data(jfile_objs, CSV_FILES)
print('csv_data len: ', len(csv_data))

# Calculate
calculate(csv_data)

exit(0)
